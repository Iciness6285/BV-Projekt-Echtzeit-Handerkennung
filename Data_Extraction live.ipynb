{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary Packages for this software to run\n",
    "import mediapipe as mp\n",
    "import cv2\n",
    "import os\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Table\n",
    "df_columns = [] #'video_id', 'label_id', 'label']\n",
    "for frame in range(1,17):\n",
    "    for landmark in range (0,21):\n",
    "        s1 = f'{\"F\"}{frame}{\"_L\"}{landmark}{\"_X\"}'\n",
    "        s2 = f'{\"F\"}{frame}{\"_L\"}{landmark}{\"_Y\"}'\n",
    "        s3 = f'{\"F\"}{frame}{\"_L\"}{landmark}{\"_Z\"}'\n",
    "        df_columns.append(s1)\n",
    "        df_columns.append(s2)\n",
    "        df_columns.append(s3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Dataframe\n",
    "df = pd.DataFrame(columns=df_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F1_L0_X</th>\n",
       "      <th>F1_L0_Y</th>\n",
       "      <th>F1_L0_Z</th>\n",
       "      <th>F1_L1_X</th>\n",
       "      <th>F1_L1_Y</th>\n",
       "      <th>F1_L1_Z</th>\n",
       "      <th>F1_L2_X</th>\n",
       "      <th>F1_L2_Y</th>\n",
       "      <th>F1_L2_Z</th>\n",
       "      <th>F1_L3_X</th>\n",
       "      <th>...</th>\n",
       "      <th>F16_L17_Z</th>\n",
       "      <th>F16_L18_X</th>\n",
       "      <th>F16_L18_Y</th>\n",
       "      <th>F16_L18_Z</th>\n",
       "      <th>F16_L19_X</th>\n",
       "      <th>F16_L19_Y</th>\n",
       "      <th>F16_L19_Z</th>\n",
       "      <th>F16_L20_X</th>\n",
       "      <th>F16_L20_Y</th>\n",
       "      <th>F16_L20_Z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 1008 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [F1_L0_X, F1_L0_Y, F1_L0_Z, F1_L1_X, F1_L1_Y, F1_L1_Z, F1_L2_X, F1_L2_Y, F1_L2_Z, F1_L3_X, F1_L3_Y, F1_L3_Z, F1_L4_X, F1_L4_Y, F1_L4_Z, F1_L5_X, F1_L5_Y, F1_L5_Z, F1_L6_X, F1_L6_Y, F1_L6_Z, F1_L7_X, F1_L7_Y, F1_L7_Z, F1_L8_X, F1_L8_Y, F1_L8_Z, F1_L9_X, F1_L9_Y, F1_L9_Z, F1_L10_X, F1_L10_Y, F1_L10_Z, F1_L11_X, F1_L11_Y, F1_L11_Z, F1_L12_X, F1_L12_Y, F1_L12_Z, F1_L13_X, F1_L13_Y, F1_L13_Z, F1_L14_X, F1_L14_Y, F1_L14_Z, F1_L15_X, F1_L15_Y, F1_L15_Z, F1_L16_X, F1_L16_Y, F1_L16_Z, F1_L17_X, F1_L17_Y, F1_L17_Z, F1_L18_X, F1_L18_Y, F1_L18_Z, F1_L19_X, F1_L19_Y, F1_L19_Z, F1_L20_X, F1_L20_Y, F1_L20_Z, F2_L0_X, F2_L0_Y, F2_L0_Z, F2_L1_X, F2_L1_Y, F2_L1_Z, F2_L2_X, F2_L2_Y, F2_L2_Z, F2_L3_X, F2_L3_Y, F2_L3_Z, F2_L4_X, F2_L4_Y, F2_L4_Z, F2_L5_X, F2_L5_Y, F2_L5_Z, F2_L6_X, F2_L6_Y, F2_L6_Z, F2_L7_X, F2_L7_Y, F2_L7_Z, F2_L8_X, F2_L8_Y, F2_L8_Z, F2_L9_X, F2_L9_Y, F2_L9_Z, F2_L10_X, F2_L10_Y, F2_L10_Z, F2_L11_X, F2_L11_Y, F2_L11_Z, F2_L12_X, ...]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 1008 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Show Dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MediaPipe to draw the hand framework over the top of hands it identifies\n",
    "drawingModule = mp.solutions.drawing_utils\n",
    "handsModule = mp.solutions.hands\n",
    "\n",
    "\n",
    "def extract_data(image):\n",
    "    # Use MediaPipe hand tracking with static_image_mode set to True\n",
    "    with handsModule.Hands(static_image_mode=True, min_detection_confidence=0.7, max_num_hands=1) as hands:\n",
    "        new_row=[]\n",
    "        j = 0\n",
    "        frame = cv2.resize(image, (640, 480))\n",
    "        a = time.time()\n",
    "        # Process the image and produce the hand framework overlay on top of the hand\n",
    "        results = hands.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # In case the system sees multiple hands, this if statement deals with that and produces another hand overlay\n",
    "        if results.multi_hand_landmarks:\n",
    "            for handLandmarks in results.multi_hand_landmarks:\n",
    "                drawingModule.draw_landmarks(frame, handLandmarks, handsModule.HAND_CONNECTIONS)\n",
    "                i = 0\n",
    "                last_idx = 0\n",
    "                for idx, landmark in enumerate(handLandmarks.landmark):\n",
    "                    while (i < idx):\n",
    "                        new_row.append(0)\n",
    "                        new_row.append(0)\n",
    "                        new_row.append(0)\n",
    "                        i = i+1\n",
    "\n",
    "                    new_row.append(landmark.x)\n",
    "                    new_row.append(landmark.y)\n",
    "                    new_row.append(landmark.z)\n",
    "                    i = idx+1\n",
    "                    last_idx = idx\n",
    "                if last_idx <= 20:\n",
    "                    for i in range(last_idx+1, 21):\n",
    "                        new_row.append(0)\n",
    "                        new_row.append(0)\n",
    "                        new_row.append(0)\n",
    "        \n",
    "        else:\n",
    "            for i in range (0,63):\n",
    "                new_row.append(0)\n",
    "            j = j+1\n",
    "        result = new_row.copy()\n",
    "        new_row.clear()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define the RNNClassifier class (as you have done before)\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout=0.5):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(self.dropout(out[:, -1, :]))\n",
    "        return out\n",
    "# Hyperparameters\n",
    "input_size = 64\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "num_classes = 10\n",
    "dropout = 0.5  # Adjust this value as needed\n",
    "\n",
    "# Initialize the model\n",
    "model = RNNClassifier(input_size, hidden_size, num_layers, num_classes, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RNNClassifier(\n",
       "  (lstm): LSTM(64, 256, num_layers=2, batch_first=True, dropout=0.5)\n",
       "  (fc): Linear(in_features=256, out_features=10, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('mps') if torch.backends.mps.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model.load_state_dict(torch.load('final_mit0.pth', map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyautogui\n",
    "def on_input(label):\n",
    "    print(label)\n",
    "    print(type(label))\n",
    "    match label:\n",
    "        case 1:  # Pushing Hand Away\n",
    "            pyautogui.click()\n",
    "        case 2: # Sliding Two Fingers Down\n",
    "            pyautogui.move(0,20,0)\n",
    "        case 5: # Sliding Two Fingers Up\n",
    "            pyautogui.move(0,-20,0)\n",
    "        case 3: # Sliding Two Fingers Left\n",
    "            pyautogui.move(-20,0,0)\n",
    "        case 4: # Sliding Two Fingers Right\n",
    "            pyautogui.move(20,0,0)\n",
    "        case 8: # Zooming In With Two Fingers\n",
    "            pyautogui.keyDown('ctrl')\n",
    "            pyautogui.scroll(100)\n",
    "            pyautogui.keyUp('ctrl')\n",
    "        case 9: # Zooming Out With Two Fingers\n",
    "            pyautogui.keyDown('ctrl')\n",
    "            pyautogui.scroll(-100)\n",
    "            pyautogui.keyUp('ctrl')\n",
    "        case default:\n",
    "            print(\"Nothing happened\")\n",
    "\n",
    "# label_dict = {0: \"Pushing hands away\",\n",
    "#               1: \"Sliding two fingers down\",\n",
    "#               2: \"Sliding two fingers left\",\n",
    "#               3: \"Sliding two fingers right\",\n",
    "#               4: \"Sliding two fingers up\",\n",
    "#               5: \"Thumbs down\",\n",
    "#               6: \"Thumbs up\",\n",
    "#               7: \"Zooming in with two fingers\",\n",
    "#               8: \"Zooming out with two fingers\"}\n",
    "\n",
    "label_dict = {0: \"Doing other things\",\n",
    "              1: \"Pushing hands away\",\n",
    "              2: \"Sliding two fingers down\",\n",
    "              3: \"Sliding two fingers left\",\n",
    "              4: \"Sliding two fingers right\",\n",
    "              5: \"Sliding two fingers up\",\n",
    "              6: \"Thumbs down\",\n",
    "              7: \"Thumbs up\",\n",
    "              8: \"Zooming in with two fingers\",\n",
    "              9: \"Zooming out with two fingers\"}\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "fourcc = cv2.VideoWriter_fourcc('m', 'p', '4', 'v')\n",
    "row_arr = []\n",
    "counter, ct = 0, 0\n",
    "n = 16\n",
    "\n",
    "starttime = time.time()\n",
    "frames_per_second = 30\n",
    "\n",
    "last_predictions = []\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    row_arr.extend(extract_data(frame))\n",
    "    counter += 1\n",
    "    \n",
    "    a = time.time()\n",
    "    if counter > n:\n",
    "        del row_arr[0:63]\n",
    "        counter -= 1\n",
    "    \n",
    "    if counter == n:\n",
    "        # model anwenden\n",
    "        x = np.concatenate([np.array(row_arr).reshape(16,63), np.zeros((16, 1))], axis=1)[None,:,:]\n",
    "        input_data = torch.tensor(x, dtype=torch.float32) \n",
    "        input_data = input_data.to(device)\n",
    "\n",
    "        if np.sum(x==0) / x.size < 0.5:     # Nur prediction wenn genug Datenpunkte vorhanden sind (>50%)\n",
    "            with torch.no_grad():           # Inference\n",
    "                output = model(input_data)[0]\n",
    "            label = torch.argmax(output).cpu().item()\n",
    "\n",
    "            # if ct%10 == 0:                # Printe nur alle 10 frames\n",
    "            #     print(\"-\"*100,\"\\n\",label, label_dict[label],\"\\n\",\"-\"*100)\n",
    "            # ct += 1\n",
    "\n",
    "            # Printe nur wenn die letzten 5 frames das gleiche predicted haben\n",
    "            # if len(last_predictions)!=0 and np.all(np.array(last_predictions) == np.array(label)):\n",
    "            #     on_input(label)\n",
    "            #     # print(\"-\"*100,\"\\n\",label, label_dict[label],\"\\n\",\"-\"*100)\n",
    "            # if len(last_predictions) >= 5:\n",
    "            #     last_predictions = last_predictions[1:]\n",
    "            # last_predictions.append(label)\n",
    "            cv2.putText(frame,str(label_dict[label]), (50,50), cv2.FONT_HERSHEY_SIMPLEX, 1, 255, thickness=5)\n",
    "            \n",
    "        \n",
    "        # uncomment following lines to get mouse movement\n",
    "        # if label and (label == 2 or label == 3 or label == 4 or label == 5):\n",
    "        #     on_input(label)\n",
    "        # elif label and (label == 1 or label == 8 or label == 9):\n",
    "        #     if ct%10 == 0:\n",
    "        #         on_input(label)\n",
    "        # ct +=1\n",
    "            \n",
    "\n",
    "\n",
    "    # Below shows the current frame to the desktop \n",
    "    cv2.imshow(\"Frame\", frame)                                                                                                                                                                                                                                                                                                                                                              \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    \n",
    "    # #Below states that if the |q| is press on the keyboard it will stop the system\n",
    "    if key == ord(\"q\"):\n",
    "        cv2.destroyAllWindows()\n",
    "        del(cap)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
